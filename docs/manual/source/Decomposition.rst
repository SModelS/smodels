.. index:: Decomposition into Simplified Models

.. |invisible compression| replace:: :ref:`invisible compression <invComp>`
.. |mass compression| replace:: :ref:`mass compression <massComp>`
.. |element| replace:: :ref:`element <element>`
.. |elements| replace:: :ref:`elements <element>`
.. |topology| replace:: :ref:`topology <topology>`
.. |topologies| replace:: :ref:`topologies <topology>`
.. |decomposition| replace:: :doc:`decomposition <Decomposition>`
.. |theory predictions| replace:: :doc:`theory predictions <TheoryPredictions>`
.. |theory prediction| replace:: :doc:`theory prediction <TheoryPredictions>`
.. |constraint| replace:: :ref:`constraint <ULconstraint>`
.. |constraints| replace:: :ref:`constraints <ULconstraint>`

.. _decomposition:

Decomposition into Simplified Models
====================================

Given an input model, the first task of SModelS is to decompose
the full model into a sum of Simplified Models (or :ref:`elements <element>` in SModelS language).
The input format can be:

* a SLHA file or
* a LHE file

(see :doc:`Basic Input <BasicInput>`).
The same decomposition method is applied for both formats:
the :ref:`decomposition <decomp>`.
This is possible by making use of the :ref:`LHE-reader <lhereader>` which translates the information of the LHE file into a 
similar form as the information of the SLHA file. Therefore, the decomposition process will be described for an SLHA file.

.. _decomp:

Decomposition
------------------------

The SLHA file describing the input model is required to contain the masses of all
the BSM states as well as their production cross sections and decay branching ratios. All the above information must follow the guidelines of the SLHA format. In particular, the cross sections also have to be included
as SLHA blocks according to the :ref:`SLHA cross section format <xsecSLHA>`.

Once the production cross sections are read from the input file, all the cross sections for *production
of two* Z\ :sub:`2`-odd *states* are stored and serve as the initial step for the decomposition. (All the other cross sections
with a different number of Z\ :sub:`2`-odd states are ignored.)
Starting from these primary mothers, all the possible decays are generated
according to the information contained in the DECAY blocks. This procedure is represented in the figure below:

.. _decomp1:

.. image:: images/decomp1B.png
   :width: 45%


Each of the possible cascade decays for each mother corresponds to a :ref:`branch <branch>`.
In order to finally generate :ref:`elements <element>`, all the branches are combined in pairs according to the production cross sections,
as shown below:

.. _decomp2:

.. image:: images/decomp2B.png
   :width: 65%


For instance, assume [b1,b2,b3] and [B1,B2] represent all possible branches (or cascade decays)
for the primary mothers A and B, respectively. Then, if a production cross section for :math:`pp \rightarrow A+B` is given in the input file, the following elements will be generated:

   [b1,B1], [b1,B2], [b2,B1], [b2,B2], [b3,B1] and [b3,B2]

Additional to the information in the DECAY blocks, SModelS also checks for the possibility of a particle to be stable.
If the particle's decay width is smaller than a certain cutoff value 'promptwidth' there will be an additional "empty decay" 
added to the ones coming from the DECAY blocks.
The decomposition process either stops when the lightest neutral BSM particle or a stable particle is reached.
Each of the :ref:`elements <element>` generated according to the procedure just described will also
store its weight, which equals its production cross section times all the branching ratios appearing in it.
In order to avoid a too large number of elements, only those satisfying a :ref:`minimum weight <minweight>` requirement are kept.
Furthermore, the elements are grouped according to their :ref:`topologies <topology>`. The final output of the
SLHA decomposition is a list of such topologies, where each topology contains a list of the elements generated during the decomposition.

* **The decomposition is implemented by the** `decompose method <theory.html#theory.decomposer.decompose>`_


.. _minweight:

Minimum Decomposition Weight
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some models may contain a large number of new states and each may have a large number of possible decays.
As a result, long cascade decays are possible and the number of elements generated by the decomposition process
may become too large, and the computing time too long.
For most practical purposes, however, elements with extremely small weights (cross section times BRs)
can be discarded, since they will fall well below the experimental limits. Therefore, during the SLHA decomposition,
whenever an element is generated with a weight below some minimum value, this element (and all elements derived from it) is ignored.
The minimum weight to be considered is given by the `sigcut <theory.html#theory.decomposer.decompose>`_ parameter
and is easily adjustable
(see `decomposer.decompose <theory.html#theory.decomposer.decompose>`_).

Note that, when computing the |theory predictions|, the weight of several |elements| can be combined together. Hence
it is recommended to set the value of `sigcut <theory.html#theory.decomposer.decompose>`_
approximately one order of magnitude below the minimum signal cross sections the experimental data can constrain.

.. _lhereader:

LHE-reader
-----------------------

More general models can be input through an LHE event file containing parton-level events, including the production of the primary
mothers and their cascade decays. 
The LHE-reader goes through the events and by doing so creates dictionaries mapping the different particles to their masses and
decays which corresponds to the DECAY and MASS blocks of the SLHA file.
The cross sections are obtained by using `crossSection.getXsecFromLHEFile <theory.html#theory.crossSection.getXsecFromLHEFile>`_ .

Notice that, for the LHE decomposition, the :ref:`elements <element>` generated are restricted to the events in the input file. Hence,
the uncertainties on the elements weights (and which elements are actually generated by the model)
are fully dependent on the Monte Carlo statistics used to generate the LHE file.
Also, when generating the events it is important to ensure that no mass smearing is applied, so the events
always contain the same mass value for a given particle.


.. _elementComp:

Compression of Elements
-----------------------


During the decomposition process it is possible to perform several simplifications on
the :ref:`elements <element>` generated. In both the :ref:`LHE <lheDecomp>` and :ref:`SLHA <slhaDecomp>`-based decompositions, two useful
simplifications are possible: :ref:`Mass Compression <massComp>` and :ref:`Invisible Compression <invComp>`.
The main advantage of performing these compressions is that the simplified :ref:`element <element>` is
always shorter (has fewer cascade decay steps), which makes it more likely to be constrained by experimental
results. The details behind the compression methods are as follows:

.. _massComp:

Mass Compression
^^^^^^^^^^^^^^^^

In case of small mass differences, the decay of an :ref:`intermediate state <odd states>` to a nearly degenerate
one will in most cases produce soft :ref:`final states <final states>`, which can not be experimentally detected.
Consequently, it is a good approximation to neglect the soft :ref:`final states <final states>` and *compress* the respective
decay, as shown below:

.. _massCompfig:

.. image:: images/massCompB.png
   :width: 80%

After the compression, only the lightest of the two near-degenerate masses are kept in the element, as shown :ref:`above <massCompfig>`.
The main parameter which controls the compression is `minmassgap <theory.html#theory.element.Element.massCompress>`_,
which corresponds to the maximum value of :math:`\epsilon`
in the :ref:`figure above <massCompfig>` to which the compression is performed:

.. math::
   & \mbox{if } |M_j - M_{j+1}| < minmassgap \rightarrow \mbox{the decay is compressed}\\
   & \mbox{if } |M_j - M_{j+1}| > minmassgap \rightarrow \mbox{the decay is NOT compressed}\\

Note that the compression is an approximation since the final
states, depending on the boost of the parent state, may not always be soft.
It is recommended to choose values of `minmassgap <theory.html#theory.element.Element.massCompress>`_
between 1-10 GeV; the default value is 5 GeV.

* **Mass compression is implemented by the** `massCompress <theory.html#theory.element.Element.massCompress>`_ **method** and can be easily turned on/off by the flag *doCompress* in the :ref:`SLHA <slhaDecomp>` or :ref:`LHE <lheDecomp>` decompositions.

.. _invComp:

Invisible Compression
^^^^^^^^^^^^^^^^^^^^^

Another type of compression is possible when
the :ref:`final states <final states>` of the last decay are invisible.
The most common example is

.. math::
   A \rightarrow \nu + B

as the last step of the decay chain, where :math:`B` is an insivible particle leading to a MET signature.
Since both the neutrino and
:math:`B` are invisible, for all experimental purposes the effective MET object is :math:`B + \nu = A`.
Hence it is possible to omit the last step in the cascade decay, resulting in a compressed element.
Note that this compression can be applied consecutively to several steps of the cascade decay if all of them
contain only invisible final states:


.. _massInvpfig:

.. image:: images/invCompB.png
   :width: 80%


* **Invisible compression is implemented by the** `invisibleCompress <theory.html#theory.element.Element.invisibleCompress>`_ **method** and can be easily turned on/off by the flag *doInvisible* in the :ref:`SLHA <slhaDecomp>` or :ref:`LHE <lheDecomp>` decompositions.


Element Sorting
---------------

In order to improve the code performance, |elements| created during |decomposition| and
sharing a commong |topology| are sorted.
Sorting allows for an easy ordering of the elements belonging to a topology and
faster element comparison.
Elements are sorted according to their branches. Branches are compared according to
the following properties:

* Number of vertices
* Number of final states in each vertex
* Final state particles (particles belonging to the same vertex are alphabetically sorted)
* Mass array

As an example, consider the three elements below:


.. _elementsorting:

.. image:: images/elSorting.png
   :width: 80%

The correct ordering of the above elements is:

Element 3 < Element 2 < Element 1


Element 1 is 'larger' than the other two since it has a larger number of vertices.
Elements 2 and 3  are identical, except for their masses. Since the mass array of
Element 3 is smaller than the one in Element 2, the former is 'smaller' than the latter.
Finally if all the branch features listed above are identical for both branches, the
elements being compared are considered to be equal.
Futhermore, the branches belonging to the same element are also sorted. Hence, if an element
has two branches:

.. math::
   element = [branch1, branch2],

it implies

.. math::
   branch1 < branch2.




* **Branch sorting is implemented by the** `sortBranches <theory.html#theory.element.Element.sortBranches>`_ **method**



